<div>For  the segmentation process, there are steps required, (i) level Detection (ii)  level classification (iii) object localization. The class label predicted for  each level is coupled with a bounding box of the images with (X, Y) coordinates  in the Level Detection method. Every input image was divided during the  segmentation process, and each pixel value was predicted and assigned a class  level. The deep learning model mask R-CNN was implemented in this research  using an algorithm that was published by He et al. in 2017. This method also enhanced  the earlier mask R-CNN, Fast R-CNN [39], and Faster R-CNN algorithms [40]. The  Faster R-CNN model uses the predicted class labels and bounding box, and two  convolutional layers that are used to predict the mask of each detected object  and show the visualization of the object location. Using Keras and TensorFlow,  our mask R-CNN network [41] loads a pre-trained ResNet101 network from the  ImageNet database. Using our training dataset of 196 photos and our validation  dataset of 30 images, ResNet101, with its 101 layers, was able to extract  useful features and provide a high degree of gland detection and classification  accuracy. The anchor box scales (8, 16, 32, 64, and 128) and anchor ratios  [0.5, 1, and 2] are adopted by the RPN (region proposal network). The square  anchor sides' lengths in pixels are 8, 16, 32, 64, and 128. This means that for each anchor  point, five square anchor boxes were made, each measuring 8 × 8, 16 × 16, 32 ×  32, 64 × 64, and 128 × 128 pixels. Using the three anchor ratios, five × three  anchor boxes were made for each anchor point. The greatest results for our  flood level detection goal would be obtained by combining anchor scale and  anchor ratios. Anchor scales ought to be adjusted in accordance with the  general shapes of the objects that have been detected. To ensure that the mask  had the same dimensions as the original image, we used the whole mask width  during the training phase. During the training, we used another  method called mini mask is generated by the bounding box of the object and  resized on a pre-defined shape. For example, a full-size mask of the image is  1024 x 1024 pixels and resized to a size of 224 x 224 pixels and it’s very  helpful for memory requirements during the training process. 5.0 sigma. The  optimizer utilizes ADAM in conjunction with root mean square propagation  (RMSProp) and adaptive gradient algorithm (AdaGrad). When there are sparse  gradients, AdaGrad performs better, but the RMSProp algorithm performs best  when there is noise. The  learning momentum was 0.9, the learning rate was 0.001, and the weight decay  was 0.0001. A considerable amount of image augmentation was used, which  included flipping 50% of all shots vertically and 50% of all photos  horizontally, applying Gaussian blur with 0.0–5.0 sigma, and applying affine  rotation by 90, 180, and 270 degrees. The architecture of the  mask R-CNN network. Image  scanning and regional proposal generation for possible objects are the first  two steps of the mask R-CNN (regional convolutional neural network) framework.  Proposal categorization, pixel-wise mask generation, and bounding box  generation come next. This particular network uses FPN for  feature extraction in addition to ResNet101 as its foundation [2].<span></span></div><div>Backbone feature maps are scanned by the region proposal  network, or RPN, which removes pointless calculations and permits the reuse of  features that have been extracted. The RPN generates two outputs for each  anchor: anchor class (foreground or background: foreground signals the likely  existence of an item) and bounding box refinement (the foreground anchor box is  resized and positioned to better match the object). </div><div>The  last round of proposals moves on to the next phase. As of right now, the RPN  generates two outputs for each ROI: bounding box refinement and class (for  objects). An ROI  pooling technique enables classifier capability by cutting a portion of a  feature map and enlarging it to a specified size. At this point, two convoluted  layers are constructed simultaneously to create a branch that creates masks for  the ROI classifier's chosen positive regions. The backbone of  Box Class Mask Input RoI Pooling for RPN Feature Map Figure 2: Fully Connected  Layers Convolution Network. Using the ROI pooling outputs, the other branch of  fully connected layers produces two values for each object: a class label and a  bounding box prediction.<span></span></div>