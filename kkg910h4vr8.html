<div><b>2.&nbsp;&nbsp;&nbsp;&nbsp; Materials and Methods</b></div><div><b>2.1. </b><b>Dataset</b></div><div>In  this paper we have used two datasets, the first dataset contained the flood  image collected from social media from Pakistan, and the second dataset that  has been used is the MS COCO dataset. We also give a brief overview of the  dataset like flood image collection and flood image annotation strategy and  generating the flood image masks.</div><div><b>2.1.1.&nbsp;&nbsp;  </b><b>Flood  dataset</b></div><div><b>&nbsp;</b>In this paper, we  evaluate flood water level on the base of the object that submerged in flood  water, during the data collection we take first is which object we are going to  consider for level detection. During selecting an object we define criteria for  data collection our criteria are based on the easy availability know dimensions  and less variation in the height of the sub-classes. When the availability of  the object is high enough frequency we can collect a large number of the  dataset. So when we know about the dimensions of the object it refers to the  fact that height length and width can be approximated and have less height  difference, for example, if we consider a car as an object, the cars come with  different types of category and should not vary in height from each other [2]. </div><div>In  this paper, our main concern is about the height of objects. Selected five  classes of objects such as Persons, cars, buses, bicycles, houses, or Buildings  based on the criteria we decided. So when we selected images for our flood  dataset at least one of these five objects was in them. After removing the  duplicated images we have only 104 images in the flood dataset. Flood water  images were only collected from the National Disaster Management Authority  (NDMA) Pakistan and social media platforms [16].</div><div><b>2.1.2.&nbsp;&nbsp;  </b><b>MS  COCO dataset</b></div><div><b>&nbsp;</b>A sizable dataset for  object detection, segmentation, and captioning is MS COCO. It was created by  Lin et al. [Lin+14] and released in 2015 [2]. The dataset is sponsored by  Mighty Ai, Facebook, Microsoft, and CVDF. It has 91 categories of things and 80  categories of objects, over 200 thousand tagged photos, and 1.5 million  instances of objects. The dataset comprises an average of 7.7 instances and 3.5  categories per image. The dataset comes in two versions, one from 2014 and the  other from 2017 [Con18]. We used the 2017 version for this job, which contains  118,000 train images and 5000 validation images [Con18]. We just use a portion  of the object categories namely, Person, Car, Bus, and Bicycle because we don't  require all of them for our work. The entire list of object categories is  included in Appendix A.2. By giving the object category ID at runtime, the task  of choosing a subset of photos is accomplished, and only images that include  this category ID are chosen for the training and validation set [2].</div><div><b>2.2. </b><b>Image Annotation</b></div><div>We  need to label the flood images in our training dataset with the flood-water  level we want to predict to train our Mask R-CNN model. We first established  the annotation method and chose which objects to take into account for the  classification task to achieve our goal of quantifying flood water level  depending on the object that is flowing in the water. The objects for this task  were selected based on the following criteria: known dimensions, convenient  availability, and little intra-class height variance. Because the object is  widely available in the real world, it was simple to gather a sizable  collection of images including the object for training and validation [2]. We  mean known size describes the height and width of the considered object that is  known, it’s simple for level estimation. In the end, we considered the low  variations of the height of the object which means many of the objects are around the same height in the  actual world. For example, Bicycle objects are hugely common in rural  areas and are also known for their size, and across different model their  height is constant. Based on the criteria we decided to select five object  classes for our research: person, car, bus, bicycle, and house. </div><div>We  chose the flood class that describes the flood water in the image along with to  all five classes that have already been discussed. In this research, we investigate 11  flood levels, ranging from 0 (no water) to 10 (the height of a person buried in  the water). Creating the training dataset is the next stage of our research,  and to do so, we must annotate the images with information on the water level.  The Visual Geometry Group developed the open-source image annotation program  VGG Image Annotator (VIA), which we used. We collected a total of  500 images during the labeling process we only labeled 130 images for training  and validation due to image quality. </div><div>    In this paper, we  used VGG Image Annotator (VIA), to label the image's shape region, and that  region was set to object submerged in water. The height of each level is then  inspired by artists who sketch the human figure using head height as the basic measure.  To map the level classes to actual flood height, we take the average height of  the human body into account and obtain the water height in centimeters (see  Table 1)</div><div></div><table>   <tbody><tr>    <td>    <div><b>Level Name<span></span></b></div>    </td>    <td>    <div><b>Range cm<span></span></b></div>    </td>    <td>    <div><b>Value nearest    integer cm<span></span></b></div>    </td>    <td>    <div><b>Feet</b></div>    </td>   </tr>   <tr>    <td>    <div>Level-0<span></span></div>    </td>    <td>    <div>No water<span></span></div>    </td>    <td>    <div>0.0<span></span></div>    </td>    <td>    <div>0</div>    </td>   </tr>   <tr>    <td>    <div>Level-1<span></span></div>    </td>    <td>    <div>0.0–1.0<span></span></div>    </td>    <td>    <div>1.0<span></span></div>    </td>    <td>    <div>0.03</div>    </td>   </tr>   <tr>    <td>    <div>Level-2<span></span></div>    </td>    <td>    <div>1.0–10.0<span></span></div>    </td>    <td>    <div>10.0<span></span></div>    </td>    <td>    <div>0.3</div>    </td>   </tr>   <tr>    <td>    <div>Level-3<span></span></div>    </td>    <td>    <div>10.0–21.25<span></span></div>    </td>    <td>    <div>21.0<span></span></div>    </td>    <td>    <div>0.6</div>    </td>   </tr>   <tr>    <td>    <div>Level-4<span></span></div>    </td>    <td>    <div>21.25–42.5<span></span></div>    </td>    <td>    <div>43.0<span></span></div>    </td>    <td>    <div>1.4</div>    </td>   </tr>   <tr>    <td>    <div>Level-5<span></span></div>    </td>    <td>    <div>42.5–63.75<span></span></div>    </td>    <td>    <div>64.0<span></span></div>    </td>    <td>    <div>2.1</div>    </td>   </tr>   <tr>    <td>    <div>Level-6<span></span></div>    </td>    <td>    <div>63.75–85<span></span></div>    </td>    <td>    <div>85.0<span></span></div>    </td>    <td>    <div>2.9</div>    </td>   </tr>   <tr>    <td>    <div>Level-7<span></span></div>    </td>    <td>    <div>85.0–106.25<span></span></div>    </td>    <td>    <div>106.0<span></span></div>    </td>    <td>    <div>3.4</div>    </td>   </tr>   <tr>    <td>    <div>Level-8<span></span></div>    </td>    <td>    <div>106.25–127.5<span></span></div>    </td>    <td>    <div>128.0<span></span></div>    </td>    <td>    <div>4.1</div>    </td>   </tr>   <tr>    <td>    <div>Level-9<span></span></div>    </td>    <td>    <div>127.5–148.75<span></span></div>    </td>    <td>    <div>149.0<span></span></div>    </td>    <td>    <div>5.0</div>    </td>   </tr>   <tr>    <td>    <div>&nbsp;Level-10</div>    </td>    <td>    <div>148.75–170.0<span></span></div>    </td>    <td>    <div>170.0<span></span></div>    </td>    <td>    <div>5.5</div>    </td>   </tr>  </tbody></table>